Reinforcement Learning Based Recommendation
System: An In-Depth Review of Models and their
Limitations

Dhaval Mehta*
Computer Engineering Department
MPSTME, NMIMS University
Mumbai - 400056, India
dhavalmehta5604@gmail.com
Nipun Dahiya
Computer Engineering Department
MPSTME, NMIMS University
Mumbai - 400056, India
nipundahiya2004@gmail.com

Ishaan Atre
Computer Engineering Department
MPSTME, NMIMS University
Mumbai - 400056, India
ishaanatre@gmail.com
Soni Sweta
Computer Engineering Department
MPSTME, NMIMS University
Mumbai - 400056, India
soni.sweta16@gmail.com

Abstract— This paper surveys the landscape of the AI model
approaches and algorithms that have been in use in recommendation
systems, which have become an essential requirement in enriched user
experiences, ranging from e-commerce to content streaming and social
media, among other areas. We focus on a few remarkable techniques:
Multi-Armed Bandit, Multi-Agent Reinforcement Learning with Deep
Deterministic Policy Gradient (DDPG), Deep Q-Network (DQN),
Proximal Policy Optimization, and Twin Delayed DDPG. Every model
is discussed in detail for the purpose of demarcating the merits and
demerits, besides shedding light on some major findings of past
literature. We enumerate some important research gaps in the domain
that relate to scalability, adaptability to dynamic user preferences, and
context-awareness. We aim to highlight the potential and limitations of
these algorithms across various applications, proposing directions for
future research to improve recommendation systems.
crucial, such as mobile-based or news recommendation
systems [4]. However, there are usually a few significant
challenges in MARL dealing with complex user behaviors
and scalability of large datasets [3][5]. MARL by using
DDPG extends the single-agent RL models since multiple
agents will recommend personal content collaboratively to a
user. Accordingly, enabling the systems for much
complicated duties such as the simultaneous prediction of the
user's preferences for various categories is possible under this
approach. To compare to the traditional RL model, MARL
works well in continuous action spaces, and the
recommendation results are much more subtle and more
accurate [6].

Keywords— Recommendation Systems, DDPG, MAB, MARL,
Algorithms, Models, PPO, T3D.
I. I NTRODUCTION
Now, the recommendation system has emerged as a
powerful tool in content personalization, everything from e-
commerce sites like Amazon to video streaming services like
Netflix or YouTube. In fact, these recommendations are a
function of data collected about the user's preferences and
interests, and with regard to the dynamics, it is the AI models
that create a difference between good and better
recommendations. The most benefiting type of machine
learning method applied for upgrading recommendation
systems has been Reinforcement Learning, a model where
agents learn from interaction with the environment. RL
models are quite effective in dynamic environments as
systems continuously adapt toward changing user behavior in
an attempt to improve their long-term engagement [1][5].
Some of the well-known RL models employed in the
recommendation system include Multi-Armed Bandit
(MAB), Deep Deterministic Policy Gradient, DDPG-based
Multi-Agent Reinforcement Learning, Deep Q-Network,
Proximal Policy Optimization, and Twin Delayed DDPG
[1][6]. Each of these models attempts to solve different
problems that arise in the recommendation systems: cold-start
problems, large state spaces, and multiple agents
recommending content simultaneously [7][16].
MAB algorithms balance exploration-exploitation trade-
off allowing the systems to balance between suggesting newer
unexplored content and optimizing w.r.t. known user
preferences [2][5]. They especially suit an environment where
rapid learning and adaptation to user preferences become
DQN models combine Q-learning with deep neural
networks in order to cope with large complex state spaces
typical in recommendation systems. Thus, Q-values for
various actions can be computed and DQN models make the
best possible choices for the optimal content in highly
dynamic and changing environments such as news or social
media platforms. However, DQN suffers from cold-start
problem, in which less data related to user is available to guide
recommendations, and a sparsity of data affecting it in a low-
feedback environment [9][10].
Another advanced RL algorithm is PPO. The algorithm
follows an actor-critic architecture that balances exploration
of new content with the exploitation of user preference that’s
known to reduce variance of updates in the policy, hence
learning more stably. Especially, PPO has proved very useful
in large-scale practice with the recommendation systems on
mobile networks that require real-time adaptation [16][17].
Finally, TD3 introduced the DDPG model with
mechanisms such as clipped double Q-learning and delayed
policy updates to avoid the overestimation of Q-values to
improve performance in continuous action spaces. Thereby,
problems featuring continuous real-time decisions about
managing energy and many complicated tasks related to
recommendation have turned out to be very efficient. These
models, while serving to significantly advance the state of
affairs in recommendation systems, are accompanied by
multiple limitations that entail scalability, cold-start issues,
and the need for more comprehensive real-world applications
[19][21].

979-8-3315-3040-2/25/$31.00 ©2025 IEEE
II. L ITERATURE R EVIEW

Table 1.1: Comparison of Reinforcement Learning Models in Recommendation Systems.

Method Pros Cons Findings
MAB (Multi- − Accelerates learning compared to A/B testing − Resource-intensive setup and − Outperforms A/B testing in speed and cost-
Armed Bandit) [1][2]. implementation challenges [1]. efficiency [1].
− Reduces exploration cost and user fatigue [3]. − Initial exploration may impact user − Improves recommendation quality with less
− Adaptable to changing preferences [4]. experience [3]. resource usage [2].
− Effective in addressing the cold-start problem − Risk of overfitting to short-term − Maintains balance between exploration and
[5]. behaviors [4]. exploitation [5].
MARL using − Effectively manages multiple recommendation − Sensitive to hyper-parameter tuning, − Evidencing Real Cooperation Between Agents
DDPG agents for personalized content delivery [6]. which can affect performance [6]. and Facilitating Superior User Experiences[6].
− Handles cold start and sparse data issues − High computational complexity; − MARL based on DDPG improves the value of
efficiently [6]. needs substantial resources for large- recommendation intentionally as compared to
− Continuous action space enhances scale applications [7]. conventional methods [7].
recommendation granularity and accuracy [6]. − Implementation can be complex and − The experimental results stated the better
− Enhances user satisfaction and engagement time-consuming [7]. performance measures i.e. RMSE and recall[7].
through real-time interactions [6][7].
DQN − Neural grids are used to estimates the Q- − Cold start issues for new users due to −  DQN works well in environments with distinct
Algorithm values, which allows for efficient computation lack of historical data [9]. action spaces, as it has issues dealing with
of them w.r.t. action selection [8]. − Data sparsity can hinder the accuracy continuous states and unless using a separate
− Is scalable to large state spaces; effective in of recommendations [9]. mechanism to directly calculate Q-values, but out
dynamic environments [9]. − High computational complexity, of the box cannot deal well enough with
especially with continual learning
− Incorporates self-supervised learning for and dynamic adjustments [10]. continuous actions. DPS is particularly better
continuous improvement [12].  suited for such environments [8].
− Integrates textual information for enhanced − Results significant improvement in classification
user and item embeddings, improving accuracy and recommendation quality when
recommendation quality [13]. benchmarked against baseline methods [10].
− Shows good continual learning and user variable
adaptation [11].
PPO − Effective in handling huge action spaces in − Requires significant computational − Achieves significant improvements in precision
(Proximal real-world recommendation systems [14][16]. resources for large-scale applications and recall on popular datasets like MovieLens
Policy − Addresses cold-start issues with hybrid [16].  [16].
Optimization) methods merging collective filtering and − Complex to implement in systems − PPO outperforms Deep Q-Learning (DQN)
content-based filtering [15][16]. requiring real-time interactions and methods in handling high-dimensional state-
− Utilizes actor-critic architecture, reducing content delivery [17].  action spaces and provides more stable updates
variance and increasing stability during the − Can suffer from performance [16][17].
degradation in dynamic
learning process [16]. environments with high variability in − Improves recommendation accuracy and reduces
− Efficient blend of recommendation and user preferences [17]. transmission costs in mobile networks through
pushing policies for users on the go [17]. proactive content pushing [17].
Twin Delayed − Eliminates Q-value overestimation through − Increased system complexity due to Performance
DDPG clipped double Q-learning [18][19] multiple networks [18] − Strong results in recommendation systems [18]
− Superior performance metrics in − Higher computational complexity vs. − Effective in energy management applications [19]
recommendation systems [20] simpler algorithms [19] − Consistently outperforms DDPG in dynamic
− Delayed policy updates prevent poor decisions − Longer training times [20] environments [20]
[18] − Complex hyperparameter tuning Trade-offs
− Target policy smoothing enhances robustness required [21] − Stability improvements vs. slower learning speed
[19][21] [18]
− Excels in continuous action spaces [18] − Balance between performance and computational
− Effective in dynamic environments like cost [19][20]
recommendations and energy systems [19][20]

A. Research Gaps
• Multi-Armed Bandit (MAB)
− Real-world validation: Although learning with MAB
models takes longer, it works well in balancing the trade-
off between exploration and exploitation. These models,
however, need to undergo more real-world validations,
particularly for dynamic environments where user
preferences may change rapidly over time [1][2]. Most
existing studies were done in static settings, leaving the
question of how well these models generalize to an
adapting environment.
− Scalability and heterogeneity: MAB models are not very
scalable for large datasets with highly diverse user
profiles. Due to the heterogeneity in user preferences,
MAB may lead to suboptimal management, resulting in
inefficiencies in recommendation quality [3].
• MARL using DDPG
− Scalability: Aside from being able to cope with a
substantial number of agents and furnish personalized
insights, MARL techniques based on DDPG should not be
intrinsically non-scalable in real-world systems. Each
agent can become a bottleneck in resource utilization and
real-time decision-making [6][7].
− Contextual performance: Further exploration is needed to
understand how user behavior and agent interactions
impact MARL with DDPG. Work with real-world datacould greatly enhance the understanding learned from
studies conducted under less realistic constraints [6].
− Extensive case studies: There's a lack of understanding
regarding how these systems can generalize from sectors
like advertising to healthcare and finance [7].
• Deep Q-Network (DQN)
− Cold start: As conventional DQN models rely on large
amounts of historical data to provide meaningful help, they
have had real difficulty in the cold start issue. Further
research is needed to address this challenge [9] [10].
− The scalability of big data, even DQN models degrade in
performance when working on larger datasets  especially
with the use-cases of application to complex areas such as
e-commerce or content streaming. Efficient solution for
processing high volume data requires scalable solutions
[9].
− Sparse data: DQN models experience a high degree of data
sparsity, particularly when the explicit feedback is scarce.
They suggested that future research should investigate
exploiting the implicit feedback of a user (e.g. browsing
history) to reenforce this limitation [12].
• Proximal Policy Optimization (PPO)
− Dynamic environment: PPO works well in static or
controlled environments, but research is needed to further
assess how it performs in dynamic environment where
user preferences change frequently [16][17].
− Scalability: While PPO scales quite nicely to large action
spaces, it hits scalability challenges for very large datasets.
Subsequent research could endeavour for such an ideal
PPO refinement technique that can be most accurate and
use least amount of computational resources available in
large-scale systems [16].
− Generalize across unobserved states: Further research is
warranted to tailor PPO even stronger in generalization
over the unseen state. Existing models are either
hardcoded or need re-training to adapt to user preferences
as they change [17].
(DDPG) has done very well in managing multiple agents and
personalized recommendations, especially in complex
continuous action environments, its scalability remains one
area to be improved so as not to get stuck in the mud when
applied to big recommendation systems. [6][7].
Deep Q-Networks (DQN): DQN performs well in computing
Q-values for a very large state space and, hence, proves to be
effective in dynamic recommendation environments.
However, they suffer from cold-start and data sparsity
complications, especially when the dataset size is extremely
large, like in news or media recommendations. Optimizing
them to run well with scarce user feedback in real-time
environments is important [9][10][12].
Proximal Policy Optimization (PPO): PPO is highly powerful
in large environments with significant action spaces, and
because of its actor-critic setup, it comes with stable
consistent policy updates. Works well in large-scale or
mobile network recommendations. Not Robust for scaling
and dealing with dynamic user behavior changes
[15][16][17].
TD3: TD3 enhances recommendation systems by improving
Q-value overestimation; also, methods like clipped double Q-
learning and delayed policy updates allow performance to be
maintained under continuous action spaces. [18][19][21].
III. P ROPOSED M ETHODOLOGY
− The methodology will focus on user feedback integration,
development of a model which has both internal and
external feedback, the recommendation will be refined
based on user queries.

• Twin Delayed DDPG (TD3)
− Scalability: TD3, just like DDPG falls apart for large
recommendation systems. Though it thrives better in
smaller, more calibrated environments, not yet tested on
either production-scale systems with millions of users and
content [19].
− Cold start and sparse data: TD3 is an RL algorithm, so it
faces issues with cold starts and when information is
limited. Therefore, it is mentioned we must improve novel
techniques to deal with sparse data and adapt the system
whenever new user behaviour happens[18][21].

B. Inference
Multi-Armed Bandits (MAB): MAB models are great
because they’re efficient with exploration costs and learn
quickly, a step up from traditional methods like A/B testing.
By balancing exploration and exploitation, they handle
recommendation tasks more effectively. But there are still
challenges, especially in real-world situations where user
preferences vary a lot and long-term engagement is key
[1][3][5].
Multi-Agent Reinforcement Learning (MARL) with DDPG:
While MARL with Deep Deterministic Policy Gradient
Fig 1.1: Recommended Methodology
The above figure (fig 1.1) is a block diagram that details the
main elements and data flow through an interactive
recommendation system incorporating user feedback
integration and a conversational AI component. The major
constituents are as follows:
− User Interface: Point where user inquiries and
interaction begins.
− Database: This holds the ready blueprint of past work
completed by the user.− Conversational AI: Converts user input and controls
conversation flow.
− NLU Engine: Analyzes and interprets natural language
inputs by the user.
− User Profile: Holds the preferences and historical record
of the user.
− Recommendation Engine: This is the core/brains that
processes these personalized recommendations
− Feedback Processor: It continuously takes, processes,
and integrates real-time feedback from the users.
− Recommendation Output: Final recommendations given
to the user.
− Explicit Feedback: Ratings given by users, likes,
dislikes, etc.
− Implicit Feedback: Indirect information given like click
through, view time, etc.
The arrows represent the flow of information between the
modules:
− Input from the user is preprocessed by the
Conversational AI and NLU Engine before it reaches the
Recommendation Engine.
− The Recommendation Engine generates the
recommendations based on the information in the User
Profile.
− Both, explicit and implicit feedbacks are processed in
real time and utilized to update the Recommendation
Engine as well as the User Profile .
− The system operates in an infinite loop, where
continually finer recommendations are yielded based on
the behavior of users and their feedback.
This architecture allows for a dynamic, responsive
recommendation system that can adapt to the preference of
users in real-time while ensuring a conversational interface
for a fulfilling experience among users.
These metrics deliver a comprehensive evaluation of how
well each model achieves in real-world recommendation
scenarios. The below figure shows the comparison of the AI
models (fig 1.3).
Performance of the recommendation models was examined
using the following metrics:
− Accuracy (Eq. 1): Measures the overall correctness of
recommendations.
 = 	          (1)
   
− Precision (Eq. 2): Evaluates the proportion of relevant
recommendations made.
"  #$
 ! = (2)
"  #$ + &'  #$
− Recall (Eq. 3): Indicates the model's ability to retrieve all
relevant items.
"  #$
+'' = (3)
"  #$ + &' ,-#$
− F1-Score (Eq. 4): Combines Precision and Recall into a
single performance metric.
 ! × +''
&1 = 2 ∗  + +'' (4)
Table 1.2 Performance Metrics: Accuracy, Precision, Recall, and F1-Score
Model Accuracy Precision Recall F1-Score
MAB 78% 76% 79% 77%
MARL 85% 83% 81% 82%
DQN 80% 78% 76% 77%
PPO 88% 85% 87% 86%
TD3 84% 82% 80% 81%

The ability to easily tackle the problem of the cold start by
preferring user's preferences
Improves trust with the user and enhances transparency in
the system through explanations and natural language.
ESULTS NALYSIS
IV. S TATISTICS , R & A
A. Model Performance Metrics
This section takes a close look at how different AI models
perform, focusing on key metrics:
− Accuracy: Measures the proportion of accurate
recommendations. For example, PPO achieved 85%
accuracy on the movie recommendation dataset, while
MAB attained 80%.
− Precision: Estimates the relevance of recommendations.
PPO achieved a precision of 82%, outperforming DQN,
which stood at 78%, indicating fewer irrelevant
suggestions by PPO.
− Recall: Mirrors the percentage of relevant items
successfully recommended. MAB outperformed DQN
with a recall of 79% compared to 76%, showing MAB's
ability to better identify relevant items.
− F1 Score: Combines precision and recall into a single
metric. PPO achieved the highest F1 score of 80%,
demonstrating well-rounded performance, compared to
DQN's 77%.
Mean Squared Error (MSE) (Eq. 5): MSE measures the
deviation between the actual and predicted preferences, and
is calculated as:
1 
345 = 6(  − ŷ ) 9 (5)
! 
:;
Where:
− y i is the actual values (user preferences or ratings),
− ŷ i is the predicted values (model output),
− n is the total number of samples (user-item interactions or
recommendations).
Table 1.3 Mean Squared Error (MSE) Comparison Across Models
Model Mean Squared Error (MSE)
MAB 0.3
MARL 0.4
DQN 0.35
PPO 0.25
TD3 0.33− Inference Time: One inference by the PPO models 15ms.
The MAB models made it in 10ms for the inferences.
The below figure shows the comparison of the AI models (fig
1.4).
The models were evaluated for their average memory and
CPU usage, providing insights into resource efficiency across
different algorithms.
− Average memory usage (Eq. 7): -

Fig 1.3: Comparison of AI Models' Performance Metrics
B. User Satisfaction and Engagement
User satisfaction and engagement are essential to
understanding the practical impact of recommendation
systems. Data includes:
− Percent Acceptance Rate of Recommendations: The
percentage of recommendations accepted by users.
MARL achieved a 70% acceptance rate, outperforming
TD3 with 65%.
− Average User Session Length: The average time users
spent interacting with the system. MARL users averaged
25 minutes per session, compared to 22 minutes for DQN.
− Click-Through Rate (CTR): The proportion of
recommended items clicked by users. MARL reached a
CTR of 12%, slightly outperforming multi-agent DDPG
(10%).
− User Retention: The percentage of users returning within
a specified timeframe. MARL had an 80% retention rate
at 30 days, while TD3 reached 76%.
The figure shows the comparison of the AI models (fig 1.4).
Multi-Armed Bandit (MAB) models optimize the balance
between exploring new recommendations and exploiting
known preferences using Eq. 6: -
+=>() = ?() + @A ln(,) (6)
! 
Where Q(a) is the estimated reward for action a, N is the total
number of trials, and n represents the number of times action
a
a was chosen. This balance ensures adaptive learning while
maximizing user satisfaction.
Fig 1.4: User Engagement and Satisfaction Across AI Models
C. Resource Usage (Memory and CPU Consumption)
This section deals with the computational resources in terms
of memory used and CPU usage. Data is available for:
− Memory Usage: PPO averaged with about 2.5 GB of
memory in training. DQN averaged with approx 1.8 GB.
− Average CPU usage: For PPO models was 70%, while for
DQN models it was 60%.
− Training Time: It took 12 hours to train PPO within a 1
million-row dataset, while MAB finished training within
9 hours.

∑ I; 3F  G- 
$-. 3F  G- = ! (7)
− Average CPU usage (Eq. 8): -

∑ I; KG G- 
$-. KG G- (%) = (8)
!
Table 1.4 Resource Consumption Metrics for RL Models and Prediction
Latency for Real-Time Applications
Model Memory Usage (GB) CPU Usage (%)
MAB 1.5 50
MARL 3 80
DQN 1.8 70
PPO 2.5 75
TD3 2 65
Fig 1.5: User Engagement and Satisfaction Across AI Models
D. Prediction Latency
Real-Time Recommendation Systems and Prediction
Latency In some applications, especially streaming services
and e-commerce, the time it takes for models to come up with
recommendations can significantly affect user experience.
− Proximal Policy Optimization(PPO): Achieved average
prediction latency of 25ms on an e-commerce dataset,
well-suited for real-time recommendation
− Deep Q-Network (DQN): Drove higher latency results of
35ms, represents poor deployment in real-time use cases.
− Multi-Agent Reinforcement Learning (MARL): MARL
showed an average prediction latency of 40ms
− Multi-Armed Bandit (MAB): Provided the lowest
latency, at 10ms or so suitable for light-weight
applications with tight response time requirements.
The below figure showcases a comparison of latency for
models (PPO, DQN, etc.). (fig 1.5).
Prediction latency was calculated to assess the real-time
efficiency of the models (Eq. 9): -
N#! = +O ! "F − +P# "F (9)Table 1.5 Prediction Latency for Real-Time Applications
Model Prediction Latency (ms)
MAB 10
MARL 40
DQN 35
PPO 25
TD3 30
Fig 1.6: Prediction Latency Comparison of AI Models
V. C ONCLUSION
In this paper, we have explored the diverse landscape of
reinforcement learning-based recommendation systems as it
applies to models that include MAB, MARL with DDPG,
DQN, PPO, and TD3. Each of the different models has
unique strengths based on the moving target of
recommendation systems since they approach problems such
as cold-start issues, large state spaces, and dynamic user
preferences. While such models possess quite substantial
potential in many domains, it has rather serious weaknesses
in scalability, adaptability, and real-world applications. The
MAB model is impressive for balancing exploration and
exploitation, but overfitting issues still exist. MARL with
DDPG offers personalization through multiagent
collaboration, but problems caused by computational
complexity. DQN manages dynamic environments
effectively but suffers from sparsity of data and cold-start
problems. PPO: efficient with a very large action space, does
worse in volatile environments. TD3 has mechanisms to
improve the performance of recommendations in continuous
action space yet lack rigorous validation in real-world
environment. To conclude, the constraints will be a base for
future research in systems. Hybrid models as well as the
proposed methodology will enhance problem related to
scalability, adaptability and sparse data in future research.
VI. R EFFERENCES
[1] N. Ravi, P. Poduval and S. Moharir, "Unreliable Multi-Armed Bandits:
A Novel Approach to Recommendation Systems," 2020 International
Conference on COMmunication Systems & NETworkS
(COMSNETS), Bengaluru, India, 2020, pp. 650-653, doi:
10.1109/COMSNETS48256.2020.9027470.
[2] M. Hejazinia, K. Eastman, S. Ye, A. Amirabadi, and R. Divvela,
“Accelerated learning from recommender systems using multi-armed
bandit,” arXiv (Cornell University), Jan. 2019,
doi: 10.48550/arxiv.1908.06158. Dorota Glowacka. 2019. Bandit
algorithms in recommender systems. In Proceedings of the 13th ACM
Conference on Recommender Systems (RecSys '19). Association for
Computing Machinery, New York, NY, USA, 574–575.
[3] Q. Wang, “The use of Bandit algorithms in intelligent interactive
recommender systems,” arXiv (Cornell University), Jan. 2021,
doi: 10.48550/arxiv.2107.00161.
[4] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A
contextual-bandit approach to personalized news article
recommendation. In Proceedings of the 19th international conference
on World wide web (WWW '10). Association for Computing
Machinery, New York, NY, USA, 661–670.
[5] Bouneffouf, D., Bouzeghoub, A., Gançarski, A.L. (2012). Exploration
/ Exploitation Trade-Off in Mobile Context-Aware Recommender
Systems. In: Thielscher, M., Zhang, D. (eds) AI 2012: Advances in
Artificial Intelligence. AI 2012. Lecture Notes in Computer Science(),
vol 7691. Springer, Berlin, Heidelberg.
[6] Suanpang, P.; Jamjuntr, P. Optimizing Electric Vehicle Charging
Recommendation in Smart Cities: A Multi-Agent Reinforcement
Learning Approach. World Electr. Veh. J. 2024, 15, 67.
[7] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. 2011.
Unbiased offline evaluation of contextual-bandit-based news article
recommendation algorithms. In Proceedings of the fourth ACM
international conference on Web search and data mining (WSDM '11).
Association for Computing Machinery, New York, USA, 297–306.
[8] X. Zhao, “DEAR: Deep Reinforcement Learning for Online
Advertising Impression in Recommender Systems”, AAAI, vol. 35,
no. 1, pp. 750-758, May 2021.
[9] S. Bangari, S. Nayak, L. Patel and K. T. Rashmi, "A Review on
Reinforcement Learning based News Recommendation Systems and
its challenges," 2021 International Conference on Artificial
Intelligence and Smart Systems (ICAIS), Coimbatore, India, 2021, pp.
260-265, doi: 10.1109/ICAIS50930.2021.9395812.
[10] Keat, E. Y., Sharef, N. M., Yaakob, R., Kasmiran, K. A., Marlisah, E.,
Mustapha, N., & Zolkepli, M. (2022). Multiobjective deep
reinforcement learning for recommendation systems. IEEE Access, 10,
65011-65027.
[11] W. Xu, X. Gao, Y. Sheng and G. Chen, "Recommendation System with
Reasoning Path Based on DQN and Knowledge Graph," 2021 15th
International Conference on Ubiquitous Information Management and
Communication (IMCOM), Seoul, Korea (South), 2021, pp. 1-8.
[12] Yu Lei, Zhitao Wang, Wenjie Li, and Hongbin Pei. 2019. Social
Attentive Deep Q-network for Recommendation. In Proceedings of the
42nd International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR'19). Association for
Computing Machinery, New York, NY, USA, 1189–1192.
[13] Wang, C., Guo, Z., Li, J., Li, G., & Pan, P. (2022). A text-based deep
reinforcement learning framework using self-supervised graph
representation for interactive recommendation. ACM/IMS
Transactions on Data Science (TDS), 2(4), 1-25.
[14] Ravichandran, R., Kumar, M., Kannan, R., & Ravi, M. (2022). RL4RS:
A real-world dataset for reinforcement learning-based recommender
system. arXiv preprint arXiv:2205.06727, 1-15.
[15] Zhao, J., Li, Q., Tang, H., & Wu, W. (2021). Reformulating
conversational recommender systems as tri-phase offline policy
learning. Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2573-2584.
[16] Padhye, V., Lakshmanan, K., & Chaturvedi, A. (2023). Proximal
policy optimization-based hybrid recommender systems for large scale
recommendations. Multimedia Tools and Applications, 82, 20079-
20100.
[17] Liu, D., & Yang, C. (2019). A deep reinforcement learning approach
to proactive content pushing and recommendation for mobile users.
IEEE Access, 7, 83120-83133.
[18] S. Bangari, S. Nayak, L. Patel and K. T. Rashmi (2021), "A Review on
Reinforcement Learning based News Recommendation Systems and
its challenges," 2021 International Conference on Artificial
Intelligence and Smart Systems (ICAIS), Coimbatore, India, pp. 260-
265, doi: 10.1109/ICAIS50930.2021.9395812.
[19] K. Sivamayil, E. Rajasekar, B. Aljafari, S. Nikolovski, S.
Vairavasundaram, and I. Vairavasundaram, “A Systematic study on
Reinforcement learning based applications,” Energies, vol. 16, no. 3,
p. 1512, Feb. 2023, doi: 10.3390/en16031512.
[20] V. Sineglazov and A. Sheruda, “Recommender systems based on
reinforced learning,” Electronics and Control Systems, vol. 2, no. 76,
pp. 46–55, Jun. 2023, doi: 10.18372/1990-5548.76.17668.
[21] X. Chen, L. Yao, J. McAuley, G. Zhou, and X. Wang, “A Survey of
Deep Reinforcement Learning in Recommender Systems: A
Systematic Review and future Directions,” arXiv (Cornell University),
Jan. 2021, doi: 10.48550/arxiv.2109.03540.
[22] Sweta, S. (2021). Recommender System to Enhancing Efficacy of E-
Learning System. In: Modern Approach to Educational Data Mining
and Its Applications. SpringerBriefs in Applied Sciences and
Technology(). Springer, Singapore.